---
title: "Preliminary analyses for Minnesota beta diversity"
output:
  pdf_document: default
  html_notebook: default
---

Description: Code for preparing and performing preliminary analysis on Minnesota 
beta diversity  
Author: Eric Le Tortorec  
Date: `r Sys.Date()`  
R version: `r R.Version()$version.string`  

##Table of contents
[Load necessary packages](#load_necessary_packages)  
[Prepare vector data](#prepare_vector_data)  
[Prepare other data](#prepare_other_data)  
[Calculate BBA points per unit](#calcualte_points_per_unit)  
[Create human land-use index with PCA](#human_land_use_pca)  
[Process and calculate human footprint index](#human_footprint)  
[Process and calculate forest loss](#forest_loss)  
[Prepare bird data](#prepare_bird_data)  
[Calculate beta diversity](#calculate_beta_div)  
[Data exploration](#data_exploration)  
[Analyse beta diversity](#analyse_beta_div)  
[Checking model residuals](#check_model_res)  

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
# set global chunk options
knitr::opts_knit$set(root.dir = normalizePath('/Users/Eric/Dropbox/Eric/Work/JKL/Theses/Matti_Hakkila/paper_4/'))
```

##Load necessary packages {#load_necessary_packages}
```{r, results = 'hide'}
library(reshape2)
library(dplyr)
library(tidyr)
library(readxl)
library(ggplot2)
library(maptools)
library(rgeos)
library(rgdal)
library(raster)
library(gstat)
library(psych)
library(lattice)
```

##Prepare vector data {#prepare_vector_data}
We prepare the vector data by joining all unit- level data together
```{r, results = 'hide', eval = FALSE}
# Read the different unit- level shapefiles
units <- readOGR('./data/minnesota_bird_data/mnbba_homogenization_data_9_22_2015/units_spatial_data/Units.shp', 'Units')
units_census <- readOGR('./data/minnesota_bird_data/mnbba_homogenization_data_9_22_2015/units_spatial_data/Units_Census.shp', 'Units_Census')
units_eco_subsection <- readOGR('./data/minnesota_bird_data/mnbba_homogenization_data_9_22_2015/units_spatial_data/Units_EcoSubsection.shp', 'Units_EcoSubsection')
units_forest_status <- readOGR('./data/minnesota_bird_data/mnbba_homogenization_data_9_22_2015/units_spatial_data/Units_ForestStatus.shp', 'Units_ForestStatus')
units_land_fire <- readOGR('./data/minnesota_bird_data/mnbba_homogenization_data_9_22_2015/units_spatial_data/Units_Landfire.shp', 'Units_Landfire')
units_prism <- readOGR('./data/minnesota_bird_data/mnbba_homogenization_data_9_22_2015/units_spatial_data/Units_PRISM.shp', 'Units_PRISM')
units_road_density <- readOGR('./data/minnesota_bird_data/mnbba_homogenization_data_9_22_2015/units_spatial_data/Units_RoadDensity.shp', 'Units_RoadDensity')
units_forest_loss <- readOGR('./data/forest_loss_3_2017/Units_ForestLoss.shp', 'Units_ForestLoss')

# Join attributes of unit- level shapefiles
units_merged <- units
units_merged <- merge(units_merged, units_eco_subsection, by = 'unit')
units_merged <- merge(units_merged, units_census, by = 'unit')
units_merged <- merge(units_merged, units_prism, by = 'unit')
units_merged <- merge(units_merged, units_forest_status, by = 'unit')
units_merged <- merge(units_merged, units_road_density, by = 'unit')
units_merged <- merge(units_merged, units_land_fire, by = 'unit')
units_merged <- merge(units_merged, units_forest_loss, by = 'unit')
units_merged$Dens_Minor <- as.numeric(units_merged$Dens_Minor)

# Save the joined data as a shapefile
writeOGR(units_merged, './data/minnesota_bird_data/mnbba_homogenization_data_9_22_2015/units_spatial_data/units_merged.shp', 'units_merged', driver = 'ESRI Shapefile', overwrite_layer = T)
```

##Prepare other data {#prepare_other_data}
```{r, results = 'hide'}
forest_status_2 <- raster('./data/minnesota_bird_data/mnbba_homogenization_data_9_22_2015/forest_status/forstat_rast2.tif')
land_fire_3 <- raster('./data/minnesota_bird_data/mnbba_homogenization_data_9_22_2015/landfire/landfire3.tif')
mnn_bba_points <- readOGR('./data/minnesota_bird_data/mnbba_homogenization_data_9_22_2015/bird_data/MNBBA_Surveys_DominantHabitat_Final.shp', 'MNBBA_Surveys_DominantHabitat_Final')
minnesota <- readOGR('./data/minnesota_bird_data/mnbba_homogenization_data_9_22_2015/units_spatial_data/Minnesota_Outline.shp', 'Minnesota_Outline')
mnn_bba_points$Date <- as.Date(mnn_bba_points$Date , "%Y/%m/%d")
```

##Calculate BBA points per unit {#calcualte_points_per_unit}
Later on we will need to determine how many forested BBA points there are per 
unit. There are three ways of doing this:

* Reclassify the forest status raster into two classes: forest and non-forest
* Reclassify the land_fire_3 raster into the same to classes
* Reclassify the land cover class of each point (this information is included 
in the BBA data) in the same two classes.

The reclassifications for the last two approaches are such that forests are:

* Lowland Deciduous Forest
* Northern Hardwoods
* Pine Forest
* Boreal Deciduous
* Oak Forest
* Parkland Deciduous Forest
* Boreal Coniferous
* Lowland Coniferous Forest
* Rural Developed Forest
* Urban Developed Forest
* Pine-Oak Barrens
* Oak Savannah

Even though reclassifying the dominant land cover class of the BBA points yields 
the least points, we will use it since it has been measured directly in the 
field. We reclassify the Reclass3_5- column from mnna_bba_points (dominant 
landcover type within 50m of the BBA point, using the reclass3 classification). 
This will be used later on to calculate the number of forested BBA points per 
unit
```{r}
mnn_bba_points$forest_bba <- mnn_bba_points$Reclass3_5
levels(mnn_bba_points$forest_bba) <- list('1' = c("Boreal Coniferous", 
  "Boreal Coniferous_10m", "Boreal Deciduous", "Boreal Deciduous_10m", 
  "Lowland Coniferous Forest", "Lowland Coniferous Forest_10m", 
  "Lowland Deciduous Forest", "Lowland Deciduous Forest_10m", 
  "Northern Hardwoods", "Northern Hardwoods_10m", "Oak Forest", 
  "Oak Forest_10m", "Oak Savannah", "Parkland Deciduous Forest_10m", 
  "Pine Forest", "Pine Forest_10m", "Pine-Oak Barrens", "Pine-Oak Barrens_10m", 
  "Rural Developed Forest_10m", "Urban Developed Forest", 
  "Urban Developed Forest_10m"), '0' = c("Boreal Lowland Grassland", 
  "Boreal Shrub Swamp", "Cropland", "Developed-High Intensity", 
  "Developed-Low Intensity", "Developed-Medium Intensity", "Lowland Herbaceous", 
  "Open Water", "Quarries-Strip Mines-Gravel Pits", "Shrub Swamp", 
  "Upland Grassland", "Upland Native Grassland", "Upland Shrub"))
mnn_bba_points$forest_bba <- as.numeric(as.character(mnn_bba_points$forest_bba))
```

We then continue by calculating how many unique BBA points fall within each 
unit. Let's count if there are points with duplicated coordinates.

```{r}
coord_counts <- plyr::count(mnn_bba_points@data, c('x', 'y'))
table(coord_counts$freq)
coord_counts_dupl <- coord_counts[coord_counts$freq > 1, ]
```

We can see that the vast majority of points have uniqe coordinates, a few have 
one duplicate, and one is present three times!

Let's continue by inspecting nearest neghbour distances for points with unique 
coordinates.
```{r}
# Select unique points based on coordinates, also sort by date
mnn_bba_points_unique <- mnn_bba_points[order(mnn_bba_points$x, 
  mnn_bba_points$y, mnn_bba_points$Date), ]
mnn_bba_points_unique <- mnn_bba_points_unique[which(!duplicated(mnn_bba_points_unique@data[c('x', 'y')], fromLast = FALSE)), ]

# Calculate distances between points
point_dist_unique <- as.data.frame(pointDistance(mnn_bba_points_unique, lonlat = FALSE))
colnames(point_dist_unique) <- mnn_bba_points_unique$ID
rownames(point_dist_unique) <- mnn_bba_points_unique$ID

# Calculate minimum distances, but leave zeros out, since they are present in 
# every row and column
point_dist_nn <- sapply(point_dist_unique, FUN = function(x) {min(x[x > 0])})
point_dist_nn <- sort(point_dist_nn)
summary(point_dist_nn)
hist(point_dist_nn, breaks = 20, main = 'Frequencies of nearest neighbour 
  distances between points', 
  xlab = 'Nearest neighbour distance (m)')
```

It's a bit hard to determine a cut-off point since there is a contimuum of 
nearest neighbour values from `r min(point_dist_nn)`m all the way to `r max(point_dist_nn)`m. 
If we look at the smallest distances we can see that there is a small spike at 
the very smallest values (around 10-20m). However, there is a steady stream of 
small frequencies up to 200m. This is a bit odd considering that we would expect 
a smallish frequency of very small distances (points that have been counted 
twice), a gap, and then clearly larger distances between independent points 
(~250m, according to the sampling protocol).
```{r}
hist(point_dist_nn[1:1000], breaks = 20, main = 'Frequencies of nearest neighbour 
  distances between points', xlab = 'Nearest neighbour distance (m)')
```

Since we are using bird counts within a 50m radius, as well as using dominant 
land cover within 50m, we will subset the points so we only include points that 
are at least 100m away from each other. We will include the earlier observation 
from each point pair, and randomy select a point if the dates are the same.
```{r}
# Select points located under 100m from each other
point_dist_nn_over <- data.frame(distance = point_dist_nn[point_dist_nn > 100])
point_dist_nn_over$ID <- as.integer(rownames(point_dist_nn_over))
point_dist_nn_under <- data.frame(distance = point_dist_nn[point_dist_nn < 100])
point_dist_nn_under$ID <- as.integer(rownames(point_dist_nn_under))
point_dist_nn_under <- dplyr::left_join(point_dist_nn_under, 
  mnn_bba_points_unique@data[c('ID', 'Date')], by = 'ID')
point_dist_nn_under <- dplyr::arrange(point_dist_nn_under, distance, Date)

point_dist_nn_under_duplc_dis_date <- point_dist_nn_under[duplicated(point_dist_nn_under[c('distance', 'Date')]) 
  | duplicated(point_dist_nn_under[c('distance', 'Date')], fromLast = TRUE), ]
point_dist_nn_under_duplc_dis <- point_dist_nn_under[!(point_dist_nn_under$ID %in% 
    point_dist_nn_under_duplc_dis_date$ID), ]
point_dist_nn_under_unique_dis <- point_dist_nn_under_duplc_dis[!duplicated(point_dist_nn_under_duplc_dis$distance, 
  fromLast = FALSE), ]

set.seed(7)
point_dist_nn_under_ID <- sapply(unique(point_dist_nn_under_duplc_dis_date$distance), 
  function(x) sample(point_dist_nn_under_duplc_dis_date$ID[point_dist_nn_under_duplc_dis_date$distance == x], 1))
point_dist_nn_under_ID <- c(point_dist_nn_under_ID, point_dist_nn_under_unique_dis$ID)
  
mnn_bba_points_unique_under <- mnn_bba_points_unique[mnn_bba_points_unique$ID %in% 
    point_dist_nn_under_ID, ]
mnn_bba_points_unique_over <- mnn_bba_points_unique[mnn_bba_points_unique$ID %in% 
    point_dist_nn_over$ID, ]

mnn_bba_points_unique_subset <- spRbind(mnn_bba_points_unique_over, mnn_bba_points_unique_under)
```

Let's continue by calculating how many unique, subsetted points there are per 
unit
```{r}
units_merged <- readOGR('./data/minnesota_bird_data/mnbba_homogenization_data_9_22_2015/units_spatial_data/units_merged.shp', 'units_merged')
# Calculate the number of unique BBA points within each unit
units_points <- over(mnn_bba_points_unique_subset, units_merged)
units_points_count <- data.frame(table(units_points$unit))
colnames(units_points_count) <- c('unit', 'point_freq')
units_points_count$unit <- as.numeric(as.character(units_points_count$unit))
units_merged <- merge(units_merged, units_points_count, by = 'unit')
units_merged$point_freq[is.na(units_merged$point_freq)] <- 0

# Summarise how many unique BBA points there are per unit
point_freq <- as.data.frame(table(units_merged$point_freq))
colnames(point_freq) <- c('points per unit', 'freq')
print.data.frame(point_freq)
hist(units_merged$point_freq, breaks = 18, xlim = c(0, 20), xlab = 'Number 
of BBA points', main = 'BBA points per unit')
```

There are a total of `r length(mnn_bba_points_unique_subset)` unique BBA points 
located at least 100m from each other, out of a total of `r length(mnn_bba_points)` 
BBA points in the data. Of the unique BBA points there are a total of `r sum(mnn_bba_points_unique_subset$forest_bba)` forested BBA points.

Let's then calculate the number of forested BBA points per unit.
```{r}
# Select forest BBA points
mnn_bba_points_forest <- mnn_bba_points_unique_subset[mnn_bba_points_unique_subset$forest_bba == 1, ]

# Calculate the number of BBA points in forested pixels within each unit
units_forest_points <- over(mnn_bba_points_forest, units_merged)
units_forest_points_count <- data.frame(table(units_forest_points$unit))
colnames(units_forest_points_count) <- c('unit', 'forest_point_freq')
units_forest_points_count$unit <- as.numeric(as.character(units_forest_points_count$unit))

units_merged <- merge(units_merged, units_forest_points_count, by = 'unit')
units_merged$forest_point_freq[is.na(units_merged$forest_point_freq)] <- 0

# Summarise how many unique forested BBA points there are per unit
forest_point_freq <- as.data.frame(table(units_merged$forest_point_freq))
colnames(forest_point_freq) <- c('forested points per unit', 'freq')
print.data.frame(forest_point_freq)
hist(units_merged$forest_point_freq, breaks = 15, xlim = c(0, 15), xlab = 'Number 
of forested BBA points', main = 'Forested BBA points per unit')
```

From the following table we can see how many units are available if a minimum 
number of forested BBA points is set.
```{r, echo = FALSE}
data.frame('Min_forested_BBA_points_per_unit' = c(1, 2, 3, 4, 5), 
  'No_of_units' = c(nrow(units_merged[units_merged$forest_point_freq >= 1, ]), 
    nrow(units_merged[units_merged$forest_point_freq >= 2, ]), 
    nrow(units_merged[units_merged$forest_point_freq >= 3, ]), 
    nrow(units_merged[units_merged$forest_point_freq >= 4, ]),
    nrow(units_merged[units_merged$forest_point_freq >= 5, ])))
```

##Create human land-use index with PCA {#human_land_use_pca}
Let's continue by selecting land-use variables that describe human land-use.

* sum of people per unit
* density of major highways per unit
* density of minor highways per unit
* density of other roads per unit
* proportion of quarries, strip mines and gravel pits
* proportion of low intensity development
* proportion of medium intensity development
* proportion of high intensity development
* proportion of cropland

```{r}
# Select variables for PCA
units_merged_pca <- dplyr::select(units_merged@data, censusSUM, Dens_Major, 
  Dens_Minor, Dens_Other, HabV15:HabV18, HabV26)
units_merged_pca$Dens_Minor[is.na(units_merged_pca$Dens_Minor)] <- 0

# Print scree plot, showing how many components to include in the PCA
VSS.scree(units_merged_pca, main = "scree plot")
```

3 components should be enough, let's perform the PCA and extract 3 factors
```{r}
# Perform PCA
human_gradient_pca <- principal(units_merged_pca, nfactors = 3, rotate="varimax",
  covar = F)
human_gradient_pca
```

We can see that the first principal component is highly correlated with 
population number, density of major highways and other roads, proportion of low 
intensity development, proportion of medium intensity development and proportion 
of high intensity development. The second principal component is correlated with 
the density of minor highways and croplands, and the third principal component 
with the proportion of quarries, strip mines and gravel pits.

The first component does a good job of summarising many aspects of human 
land-use, and captures 58% of all the variation (and 67% of the first three 
components), while the next two are more specific and clearly capture less 
variation.

Let's join the first component to the merged data
```{r}
units_merged$human_pca <- human_gradient_pca$scores[, 1]
```

##Process and calculate human footprint index {#human_footprint}
We continue by looking into calculating an index describing the intensity of 
the human footprint.

Then we read, clip and reproject the human footprint data so that average values 
per unit can be calculated. The first part of the code below reads, clips and 
reprojects the human footprint data. For some reason raster::projectRaster 
cannot reproject between WGS84 and UTM zone 15, even if the raster has been 
clipped to the spatial limits of zone 15. Thus, the clipped raster is saved as 
a geotiff, and reprojected with gdal in the terminal.
```{r, eval = FALSE}
human_footprint_orig <- raster('./data/last_of_the_wild/human_footprint_2/hfp_n_amer/dblbnd.adf')
crs(human_footprint_orig) <- CRS("+init=epsg:4326")

minnesota_extent <- as(extent(minnesota), 'SpatialPolygons')
crs(minnesota_extent) <- crs(minnesota)

minnesota_extent_wgs84 <- spTransform(minnesota_extent, CRS = CRS('+init=epsg:4326'))
human_footprint_minnesota <- crop(human_footprint_orig, minnesota_extent_wgs84)

writeRaster(human_footprint_minnesota, filename = './Data/last_of_the_wild/human_footprint_2/human_footprint_minnesota.tif', overwrite =T)
###gdalwarp -s_srs '+init=epsg:4326' -t_srs '+init=epsg:26915' human_footprint_minnesota.tif human_footprint_utm15n.tif
```

We then read the reprojected data and calculate the average human footprint 
value per unit, and merge it to the unit- level data
```{r}
#human_footprint <- raster('./data/last_of_the_wild/human_footprint_2/human_footprint_utm15n.tif')
#units <- units[order(units$unit, decreasing = FALSE), ]
#
#units_hfp <- extract(human_footprint, units, fun = mean, na.rm = T)
#units_hfp <- data.frame(unit = seq(1, 617, 1), human_footprint = units_hfp)
#write.csv(units_hfp, './data/last_of_the_wild/human_footprint_2/units_hfp.csv', 
#  row.names = FALSE)

# Join human footprint data to units_merged
units_hfp <- read.csv('./data/last_of_the_wild/human_footprint_2/units_hfp.csv')
units_merged <- merge(units_merged, units_hfp, by = 'unit')
```

##Process and calculate forest loss {#forest_loss}
In addition to the index describing human land use we want to study if forest 
loss has an impact on beta diversity. We have forest loss data from Minnesota, 
which is derived from Hansen et al. 2013 *. The data has proportion 
forest loss between 2001-2014 per unit. Note that forest fires are also 
included! Ed said that data separating the two types of forest loss 
might be available in 6 months to 1 year, but this is too long. The forest loss 
data has previously been joined to the unit- level data.  

*Hansen MC, Potapov PV, Moore R, Hancher M, Turubanova SA, Tyukavina A, et 
al. High-Resolution Global Maps of 21st-Century Forest Cover Change. Science. 
2013 Nov 15;342(6160):850–3. 

##Prepare bird data {#prepare_bird_data}
Before doing anything else, we subset the data by setting a minumum number of 
forested BBA points per unit.
```{r}
# Set minimum number of forested unique BBA points per unit
################
min_points <- 3
################
units_merged_subset <- units_merged[units_merged$forest_point_freq >= min_points, ]
```

We can see from the following map that units with forested BBA points are very 
much concentrated in the northern parts of the state:
```{r, echo = FALSE}
plot(minnesota)
plot(units_merged_subset, add = TRUE)
```

Let's continue by loading the bird data and printing all unique bird species in 
the data.
```{r}
library(readxl)
bird_data_whole <- read_excel('./data/minnesota_bird_data/mnbba_homogenization_data_9_22_2015/bird_data/Homogenization_All_BirdData.xlsx')
sort(unique(bird_data_whole$common))
```

We filter the bird data so that we only include forested BBA points with unique 
coordinates located in units with a minimum of `r min_points` forested BBA points, and 
leave out all unidentified species.
```{r}
# Identify common units between the subsetted units data and the bird data (not 
# all of the units are included in the bird data, unit 617 is missing)
common_units <- dplyr::inner_join(data.frame(unit = units_merged_subset$unit), data.frame(unit = unique(bird_data_whole$unit)), by = 'unit')
unique_coord_id <- mnn_bba_points_forest@data[c('ID', 'x', 'y')]
#colnames(bird_data_whole)

bird_points_species <- dplyr::select(bird_data_whole, ID, unit, common, Sum_Inside50m) %>% 
  dplyr::filter(ID %in% unique_coord_id$ID) %>% 
  dplyr::filter(unit %in% common_units$unit) %>% 
  dplyr::filter(!grepl('Unidentified', common)) %>% 
  dplyr::arrange(ID, common)

bird_points_species_wide <- dplyr::select(bird_points_species, -unit) %>% 
  tidyr::spread(ID, Sum_Inside50m)
bird_points_species_wide[is.na(bird_points_species_wide)] <- 0
bird_points_species_wide <- as.data.frame(bird_points_species_wide)
rownames(bird_points_species_wide) <- bird_points_species_wide$common
bird_points_species_wide <- bird_points_species_wide[2:ncol(bird_points_species_wide)]
bird_points_species_wide <- bird_points_species_wide[colSums(bird_points_species_wide) > 0]
```

##Calculate beta diversity {#calculate_beta_div}
Next we calculate beta diversity from the BBA points with the Rao quadratic 
entropy index. The Jost correction is used to correct lower than expected beta 
diversity values. We us the the Rao function, written by Francesco Bello et al. *  

We write a loop to calculate beta diversity between forested BBA points within 
each unit that contains previously specified minimum number of forested BBA 
points. We use mean additive beta per unit (gamma minus mean alpha)

*De Bello F, Lavergne S, Meynard CN, Lepš J, Thuiller W. The partitioning of 
diversity: showing Theseus a way out of the labyrinth: Theseus and the 
partitioning of diversity. Journal of Vegetation Science. 2010;21(5):992–1000.
```{r}
source('./Code/Rao.r')

beta_unit_list <- vector("list", base::nrow(common_units))
counter <- 0
for (unit in as.vector(common_units$unit)) {
  counter <- counter + 1
  #print(paste0('Unit: ', unit))
  #print(paste0('Counter: ', counter))
  bird_unit <- bird_points_species[bird_points_species$unit == unit, ]
  bird_unit_wide <- dplyr::select(bird_unit, -unit) %>% 
    tidyr::spread(ID, Sum_Inside50m)
  bird_unit_wide[is.na(bird_unit_wide)] <- 0
  bird_unit_wide <- as.data.frame(bird_unit_wide)
  rownames(bird_unit_wide) <- bird_unit_wide$common
  bird_unit_wide <- bird_unit_wide[2:ncol(bird_unit_wide)]
  bird_unit_wide <- bird_unit_wide[colSums(bird_unit_wide) > 0]
  funct_matrix <- matrix(data = 0, nrow = nrow(bird_unit_wide), 
    ncol = nrow(bird_unit_wide))
  phylo_matrix <- matrix(data = 0, nrow = nrow(bird_unit_wide), 
    ncol = nrow(bird_unit_wide))
  bird_div <- Rao(sample = bird_unit_wide, dfunc = funct_matrix, 
    dphyl = phylo_matrix, weight = FALSE, Jost = TRUE, structure = NULL)
  #########
  beta_unit_list[[counter]] <- bird_div$TD$Beta_add
  #########
}

units_beta <- data.frame(beta = unlist(beta_unit_list), unit = common_units$unit)
units_merged_beta <- merge(units_merged, units_beta, by = 'unit')
units_merged_beta <- units_merged_beta[!is.na(units_merged_beta$beta), ]
```

```{r, echo = FALSE, results = FALSE, eval = FALSE}
funct_matrix <- matrix(data = 0, nrow = nrow(bird_points_species_wide), 
  ncol = nrow(bird_points_species_wide))
phylo_matrix <- matrix(data = 0, nrow = nrow(bird_points_species_wide), 
  ncol = nrow(bird_points_species_wide))

source("./Code/Rao.r")
bird_div <- Rao(sample = bird_points_species_wide, dfunc = funct_matrix, 
  dphyl = phylo_matrix, weight = FALSE, Jost = TRUE, structure = NULL)
bird_div_beta_add <- as.matrix(bird_div$TD$Pairwise_samples$Beta_add)
bird_div_beta_add[lower.tri(bird_div_beta_add, diag = TRUE)] <- NA
bird_div_beta_add_list <- reshape2::melt(bird_div_beta_add) %>% 
  dplyr::select(unit_1 = Var1, unit_2 = Var2, beta = value) %>% 
  na.omit() %>% dplyr::arrange(unit_1, unit_2)

#aaa <- bird_div_beta_add_list[1:10, ]
#bbb$hfp_diff <- apply(aaa, 1, FUN = function(x) {print(x[1])})
#bbb$hfp_diff <- apply(aaa, 1, FUN = function(x) {print(x[1])})
#bird_div_beta_prop <- as.data.frame(as.matrix(bird_div$TD$Pairwise_samples$Beta_prop))
#bird_div_beta_add[1:10, 1:10]
#bird_div_beta_prop[1:10, 1:8]
#bird_div_alpha <- data.frame(alpha = bird_div$TD$Alpha)
#bird_div_alpha$ID <- as.integer(rownames(bird_div_alpha))
```

##Data exploration {#data_exploration}
Before starting the analyses we do some data exploration to make sure we don't 
have outliers, colinearity etc. These explanatory analyses are from Zuur et. al 
2010 *  

*Zuur AF, Ieno EN, Elphick CS. A protocol for data exploration to avoid common 
statistical problems: Data exploration. Methods in Ecology and Evolution. 2010 
Mar;1(1):3–14. 

```{r}
# Define explanatory variables
explanatory_variables <- dplyr::select(units_merged_beta@data, unit, 
  'human_footprint' = human_footprint, human_pca, 
  'forest_loss' = allV1, 
  #'major_highways' = Dens_Major, 'minor_highways' = Dens_Minor, 
  'other_roads' = Dens_Other, 'mean_temperature' = tempMN, 
  'mean_precipitation' = precMN, 'forest_point_freq' = forest_point_freq)
```

1. Outliers
```{r}
explanatory_variables_long <- dplyr::select(explanatory_variables, -unit) %>% 
  tidyr::gather()

ggplot(explanatory_variables_long, aes(factor(0),value)) +
  geom_boxplot() +  facet_wrap(~key, scales = "free", ncol = 4) + 
  theme(axis.text.x=element_blank(), axis.title.x=element_blank(), 
    axis.title.y = element_blank())
```

We can see that there is one potential outlier in the forest loss variable. This 
is from unit number 38. Looking at the forest loss layer shows that the unit in 
question is right at the centre of the 2011 Pagami Creek fire, which covered 
almost half of the area of the unit (and a fifth of unit 38). In addition, units 
12 and 559, located northeast from Pagami creek seems to be covered by a forest 
fire, most probably the 2007 Ham Lake fire. We should consider leaving these out.

The units with the highest forest loss values are:
```{r}
dplyr::select(units_merged_beta@data, unit, 'forest_loss' = allV1) %>% 
  dplyr::arrange(desc(forest_loss)) %>% head(n = 10)
```

Another potential outlier is unit 115 with a total of 18 forested BBA points. 
Let's print the units with the highest numbers of forested BBA points:
```{r}
dplyr::select(units_merged_beta@data, unit, 'forest_point_freq' = forest_point_freq) %>% 
  dplyr::arrange(desc(forest_point_freq)) %>% head(n = 10)
```

```{r}
dplyr::select(units_merged_beta@data, unit, 'other_roads' = Dens_Other) %>% 
  dplyr::arrange(desc(other_roads)) %>% head(n = 10)
```

This is a slightly trickier situation. The two highest values are from two units 
that are situated in Minneapolis (there are units with much higher values but 
they have been filtered out previously). We would leave these two units out 
because they are heavily urbanised, but they also have at least `r min_points` 
forested BBA points. Since we do not have any clear reason to drop them we 
probably just keep them.

2. Check reponse variable for excess of zeros
```{r}
hist(units_merged_beta$beta, main = 'Distribution of beta values', xlab = 'Mean beta per unit')
```

We shouldn't have problems here. Zeros don't dominate the data, and the beta 
diversity values are more or less normally distributed.

3. Colinearity of explanatory variables
```{r, echo = FALSE, results = 'hide'}
# Functions for printing pair plots 
panel.hist <- function(x, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(usr[1:2], 0, 1.5) )
  h <- hist(x, plot = FALSE)
  breaks <- h$breaks; nB <- length(breaks)
  y <- h$counts; y <- y/max(y)
  rect(breaks[-nB], 0, breaks[-1], y, col="cyan", ...)
}
panel.cor <- function(x, y, digits=2, prefix="", cex.cor, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  r <- cor(x, y, method="pearson")
  txt <- format(c(r, 0.123456789), digits=digits)[1]
  txt <- paste(prefix, txt, sep="")
  if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)
  text(0.5, 0.5, txt, cex = 1.5)
}
```

```{r, fig.width = 8, fig.height = 8}
# Pair plots of explanatory variables
pairs(as.data.frame(dplyr::select(explanatory_variables, -unit)), cex.labels = 1.2, font.labels=1.6, 
  diag.panel=panel.hist, upper.panel= panel.cor)
```

```{r}
source('./code/HighstatLib.r')
explanatory_variable_vifs <-corvif(dplyr::select(explanatory_variables, -unit))
```

We can see that there is a strongish correlation between a few of the variables, 
and a few strong variance inflation factors (VIF). The culprit here is the PCA 
of human influence, which is highly correlated with other roads (pretty obvious, 
considering that other roads was included in the PCA), as well as the human 
footprint. This is good news since the human footprint does a good job of 
describing human influence, so we can drop the PCA of human influence from our 
analyses.

4. Relationships between response and explanatory variables
```{r}
# Explanatory variables in vector format
x <- as.vector(as.matrix(dplyr::select(explanatory_variables, -unit)))
# Response variable in vector format, repeat once for each explanatory variable
y <- rep(units_merged_beta$beta, length(names(explanatory_variables)))
# Explanatory variable ID's as factors
x_id <- factor(rep(names(dplyr::select(explanatory_variables, -unit)), 
  each = length(units_merged_beta$beta)))

# Plot y vs x, and use a loess smoother to give some idea of the relation
xyplot(y ~ x | x_id, col = 1,
  strip = function(bg = 'white', ...) strip.default(bg = 'white', ...),
  scales = list(alternating = T,
                x = list(relation = "free"),
                y = list(relation = "same")),
  xlab = "Explanatory variables",
  par.strip.text = list(cex = 0.8),
  ylab = "Mean beta diversity per unit",
  panel = function(x, y, subscripts, ...){
    panel.grid(h = -1, v = 2)
    panel.points(x, y, col = 1, pch = 16)
    panel.loess(x, y, col = 1, lwd = 2)
    })
```

There are two interesting relations. First, the outlier in forest loss really 
sticks out. Also, there is a very clear pattern of increasing mean beta 
diversity per unit with an increasing number of forested BBA points per unit.

##Analyse beta diversity {#analyse_beta_div}
Let's drop the four units clearly impacted by forest fires. Since beta diversity 
values are calculated within units we can just drop them, instead of having to 
recalculate diversity measures.
```{r}
units_merged_beta_subset <- units_merged_beta[!(units_merged_beta$unit %in% 
    c(39, 12, 38, 559)), ]
```

We now have a total of `r length(unique(units_merged_beta_subset$unit))` 
units to work with.

We then continue by starting to construct a model where the response variable is 
mean beta diversity per unit, and the explanatory variables are, for example, 
human footprint, forest loss etc. We also take the number of forest points per 
unit into account as a covariate.

```{r}
model_variables <- dplyr::inner_join(units_merged_beta_subset@data[c('unit', 'beta')], 
  dplyr::select(explanatory_variables, -human_pca))
m <- lm(beta ~ human_footprint + forest_loss + other_roads + mean_temperature + 
    mean_precipitation + forest_point_freq, data = model_variables)
summary(m)
```

##Checking model residuals {#check_model_res}
1. Homogeneity of residuals
```{r}
#get unstandardized predicted and residual values
unstandardizedPredicted <- predict(m)
unstandardizedResiduals <- resid(m)
#get standardized values
standardizedPredicted <- (unstandardizedPredicted - mean(unstandardizedPredicted)) / sd(unstandardizedPredicted)
standardizedResiduals <- (unstandardizedResiduals - mean(unstandardizedResiduals)) / sd(unstandardizedResiduals)
#create standardized residuals plot
plot(standardizedPredicted, standardizedResiduals, main = "Standardized Residuals Plot", xlab = "Standardized Predicted Values", ylab = "Standardized Residuals")
#add horizontal line
abline(0,0)
```

2. Normality of residuals
```{r}
hist(standardizedResiduals, freq = FALSE, main = 'Histogram of model residuals', xlab = 'Residuals')
curve(dnorm, add = TRUE)
#get probability distribution for residuals
probDist <- pnorm(standardizedResiduals)
#create PP plot
plot(ppoints(length(standardizedResiduals)), sort(probDist), main = "PP Plot", xlab = "Observed Probability", ylab = "Expected Probability")
#add diagonal line
abline(0,1)
```

3. Independence of the response variable

Let's see how the semivariance of the residuals change over physical distance. 
```{r, eval = FALSE}
# Grab unit coordinates, add unit names to them, join to residuals data, 
# create semivariogram
unit_centroids <- coordinates(units_merged_beta_subset)
unit_dist <- as.data.frame(pointDistance(unit_centroids, lonlat = FALSE))
max_unit_dist <- max(unit_dist)
min_unit_dist <- min(sapply(unit_dist, FUN = function(x) {min(x[x > 0])}))
cutoff_dist <- sqrt(2) * 0.5 * max_dist

m_resid_x_y <- data.frame(unit = model_variables$unit, resid = m$residuals, 
  x = unit_centroids[, 1], y = unit_centroids[, 2])
resid_gstat <- gstat(id = "resid", formula = resid ~ x + y, location = ~ x + y, 
  data = m_resid_x_y)
resid_variogram <- variogram(resid_gstat, width = (cutoff_dist / 25), 
  cutoff = cutoff_dist)
resid_variogram
plot(resid_variogram, main = 'Semivariogram of bird alpha diversity', 
  xlab = 'Distance (m)')

#aaa <- fit.variogram(resid_variogram, model = vgm(1, "Exp", 1, 0))
```

```{r}
library(spdep)
# Test for spatial autocorrelation with Moran's I
# Determine adjacency of polygons
aaa <- poly2nb(units_merged_beta_subset, row.names = units_merged_beta_subset$unit)
# Transform adjacency into into a spatial weights matrix
bbb <- nb2listw(aaa, style = 'B', zero.policy = TRUE)
# Calculate p-value based on approximation of normal distribution
moran.test(units_merged_beta_subset$human_footprint, bbb, randomisation = FALSE, alternative = 'two.sided', zero.policy = TRUE)
# Calculate p-value without approximation of normal distribution
moran.test(units_merged_beta_subset$human_footprint, bbb, alternative = 'two.sided', zero.policy = TRUE)
# Calculate test with permutations
ccc <- moran.mc(units_merged_beta_subset$human_footprint, bbb, 99, zero.policy = TRUE)
# Plot the results. Modified from Olli Lehtonen's material, very poor documentation
ddd <- ccc$res[1: length(ccc$res - 1)]
eee <- density(ddd)
plot(eee, main = "Moran's I perutation test", xlab = 'Reference distribution', lwd = 2, col = 2)
hist(ddd, freq = FALSE, add = TRUE)
abline(v = ccc$statistic, lwd = 2, col = 4)
```

```{r}
# Test for local spatial autocorrelation with LISA (local indicators of spatial 
# association)
# Determine adjacency of polygons
aaa <- poly2nb(units_merged_beta_subset, row.names = units_merged_beta_subset$unit)
# Transform adjacency into into a spatial weights matrix
bbb <- nb2listw(aaa, style = 'B', zero.policy = TRUE)

ccc <- moran.plot(units_merged_beta_subset$human_footprint, bbb, zero.policy = TRUE, 
  xlab = 'Residuals', ylab = 'Spatially lagged residuals')

infl <- apply(nci$is.inf, 1, any)
x <- sids$MEASK06
lhx<-cut(x, breaks=c(min(x), mean(x), max(x)), labels=c("L","H"),include.lowest=TRUE)
wx<-lag(sids_nbq_w,sids$MEASK06)
lhwx<-cut(wx, breaks=c(min(wx), mean(wx), max(wx)),labels=c("L","H"),include.lowest=TRUE)
lhlh<-interaction(lhx,lhwx,infl, drop=TRUE)
cols <-rep(1, length(lhlh))
> cols[lhlh== "L.L.TRUE"] <-1
> cols[lhlh== "H.L.TRUE"] <-2
cols[lhlh== "L.H.TRUE"] <-3
cols[lhlh== "H.H.TRUE"] <-4
``

...to be continued...

Options to study human influence vs. beta div
-Individual pairwise beta diversities (a lot of them!):
  *explain difference per pair with differences in explanatory variables BUT we 
  are dealing with individual points, not units. It does not seem sensible to 
  look at differences between units, but between the surroundings of points...
  *take units into account as random effects (but there are usually two units 
  per pair, do we just take both into account?)
  *need to take distance into account
-Mean beta per unit (additive or proportional)
  explain mean beta per unit with explanatory variables per unit
-